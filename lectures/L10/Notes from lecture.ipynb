{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisationProblem:\n",
    "    def __init__(self, parameters):\n",
    "        self.x = self.random_guess()\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        pass\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        pass\n",
    "    \n",
    "    def random_guess(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimiser:\n",
    "    def __init__(self, optimisation_problem, step_length, n_iter, tol):\n",
    "        self.optimisation_problem = optimisation_problem\n",
    "        self.step_length = step_length\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "    \n",
    "    def gradient_step(self, x):\n",
    "        return x - self.step_length*self.optimisation_problem.gradient(x)\n",
    "    \n",
    "    def optimise(self):\n",
    "        for _ in range(self.n_iter):\n",
    "            x = self.optimisation_problem.x\n",
    "            self.optimisation_problem.x = self.gradient_step(x)\n",
    "            \n",
    "            if self.check_convergence():\n",
    "                break\n",
    "    \n",
    "    def check_convergence(self):\n",
    "        x = self.optimisation_problem.x\n",
    "        gradient = self.optimisation_problem.gradient(x)\n",
    "        return np.linalg.norm(gradient) < self.tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionProblem:\n",
    "    def __init__(self, data_matrix, target_vector):\n",
    "        self.data_matrix = data_matrix\n",
    "        self.target_vector = target_vector\n",
    "        self.x = self.random_guess_at_minimiser()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.sum((self.data_matrix@x - self.target_vector)**2)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return self.data_matrix.T@(self.data_matrix@x - self.target_vector)\n",
    "    \n",
    "    def random_guess_at_minimiser(self):\n",
    "        return np.random.standard_normal(\n",
    "            (self.data_matrix.shape[1], self.target_vector.shape[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us test these classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.random.standard_normal((100, 5))\n",
    "\n",
    "weights = np.random.standard_normal((5, 1))\n",
    "\n",
    "target_vector = data_matrix@weights\n",
    "noisy_target = target_vector + np.random.standard_normal(target_vector.shape)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative error between the true weights and the discovered weights are:\n",
      "[[-0.00090508]\n",
      " [-0.00556443]\n",
      " [ 0.00077474]\n",
      " [-0.00161982]\n",
      " [-0.00127895]]\n"
     ]
    }
   ],
   "source": [
    "linear_regression_problem = LinearRegressionProblem(data_matrix, noisy_target)\n",
    "\n",
    "optimisation_problem = GradientDescentOptimiser(\n",
    "    optimisation_problem=linear_regression_problem,\n",
    "    step_length=0.01,\n",
    "    n_iter=1000,\n",
    "    tol=1e-8  # 10^{-8}=1e-8\n",
    ")\n",
    "\n",
    "optimisation_problem.optimise()\n",
    "\n",
    "print('The relative error between the true weights and the discovered weights are:')\n",
    "print((weights - linear_regression_problem.x)/weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressionProblem:\n",
    "    def __init__(self, data_matrix, target_vector, regularisation_penalty):\n",
    "        self.data_matrix = data_matrix\n",
    "        self.target_vector = target_vector\n",
    "        self.x = self.random_guess_at_minimiser()\n",
    "        self.regularisation_penalty = regularisation_penalty\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        unregularised_loss = np.sum((self.data_matrix@x - self.target_vector)**2)\n",
    "        regularisation_loss = self.regularisation_penalty*np.linalg.norm(x)**2\n",
    "        return unregularised_loss + regularisation_loss\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        unregularised_gradient = self.data_matrix.T@(self.data_matrix@x - self.target_vector)\n",
    "        regularisation_gradient = self.regularisation_penalty*x\n",
    "        return unregularised_gradient + regularisation_gradient\n",
    "    \n",
    "    def random_guess_at_minimiser(self):\n",
    "        return np.random.standard_normal(\n",
    "            (self.data_matrix.shape[1], self.target_vector.shape[1])\n",
    "        )\n",
    "\n",
    "class LinearRegressionProblem(RidgeRegressionProblem):\n",
    "    def __init__(self, data_matrix, target_vector):\n",
    "        super().__init__(\n",
    "            data_matrix=data_matrix,\n",
    "            target_vector=target_vector, \n",
    "            regularisation_penalty=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative error between the true weights and the discovered weights are:\n",
      "[[-0.00090508]\n",
      " [-0.00556443]\n",
      " [ 0.00077474]\n",
      " [-0.00161982]\n",
      " [-0.00127895]]\n"
     ]
    }
   ],
   "source": [
    "linear_regression_problem = LinearRegressionProblem(data_matrix, noisy_target)\n",
    "\n",
    "optimisation_problem = GradientDescentOptimiser(\n",
    "    optimisation_problem=linear_regression_problem,\n",
    "    step_length=0.01,\n",
    "    n_iter=1000,\n",
    "    tol=1e-8  # 10^{-8}=1e-8\n",
    ")\n",
    "\n",
    "optimisation_problem.optimise()\n",
    "\n",
    "print('The relative error between the true weights and the discovered weights are:')\n",
    "print((weights - linear_regression_problem.x)/weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative error between the true weights and the discovered weights are:\n",
      "[[ 9.61645523e-05]\n",
      " [-2.40643614e-03]\n",
      " [ 1.80312096e-03]\n",
      " [-2.54639353e-04]\n",
      " [-5.31864793e-04]]\n"
     ]
    }
   ],
   "source": [
    "linear_regression_problem = RidgeRegressionProblem(data_matrix, noisy_target, 0.1)\n",
    "\n",
    "optimisation_problem = GradientDescentOptimiser(\n",
    "    optimisation_problem=linear_regression_problem,\n",
    "    step_length=0.01,\n",
    "    n_iter=1000,\n",
    "    tol=1e-8  # 10^{-8}=1e-8\n",
    ")\n",
    "\n",
    "optimisation_problem.optimise()\n",
    "\n",
    "print('The relative error between the true weights and the discovered weights are:')\n",
    "print((weights - linear_regression_problem.x)/weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
